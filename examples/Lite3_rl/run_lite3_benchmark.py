#!/usr/bin/env python3
"""
Lite3 ONNX ç­–ç•¥æ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•ä¸åŒè®¾å¤‡ï¼ˆMUSA GPU, CUDA GPU, CPUï¼‰çš„æ¨ç†æ€§èƒ½

ä½¿ç”¨æ–¹æ³•:
    python run_lite3_benchmark.py --device musa --warmup 100 --iterations 1000
    python run_lite3_benchmark.py --device auto --compare_all  # å¯¹æ¯”æ‰€æœ‰å¯ç”¨è®¾å¤‡
"""

import sys
import os
import time
import argparse
import numpy as np
from tqdm import tqdm
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

from envs.legged_gym.utils.onnx_policy import load_onnx_policy, ONNXPolicy
from orca_gym.utils.device_utils import get_gpu_info, print_gpu_info
import psutil
import gc


class Lite3PerformanceBenchmark:
    """Lite3 ONNX æ€§èƒ½æµ‹è¯•ç±»"""
    
    def __init__(self, policy: ONNXPolicy, device_name: str):
        self.policy = policy
        self.device_name = device_name
        self.inference_times = []
        self.memory_usage = []
        
    def warmup(self, num_warmup: int = 100):
        """é¢„çƒ­ï¼šè¿è¡Œå¤šæ¬¡æ¨ç†ä»¥ç¨³å®šæ€§èƒ½"""
        print(f"ğŸ”¥ é¢„çƒ­ä¸­... ({num_warmup} æ¬¡æ¨ç†)")
        dummy_obs = np.random.randn(45).astype(np.float32)
        
        with tqdm(total=num_warmup, desc="  é¢„çƒ­è¿›åº¦", unit="iter", ncols=80, leave=False) as pbar:
            for _ in range(num_warmup):
                _ = self.policy(dummy_obs)
                pbar.update(1)
        
        print("âœ“ é¢„çƒ­å®Œæˆ")
    
    def benchmark_single_inference(self, num_iterations: int = 1000):
        """å•æ¬¡æ¨ç†æ€§èƒ½æµ‹è¯•"""
        print(f"\nğŸ“Š å•æ¬¡æ¨ç†æ€§èƒ½æµ‹è¯• ({num_iterations} æ¬¡è¿­ä»£)")
        print("=" * 80)
        
        dummy_obs = np.random.randn(45).astype(np.float32)
        self.inference_times = []
        
        # è®°å½•åˆå§‹å†…å­˜
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æµ‹è¯•å¾ªç¯ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        with tqdm(total=num_iterations, desc="  æµ‹è¯•è¿›åº¦", unit="iter", ncols=80,
                  bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
            for i in range(num_iterations):
                # å¼€å§‹è®¡æ—¶
                start_time = time.perf_counter()
                
                # æ¨ç†
                _ = self.policy(dummy_obs)
                
                # ç»“æŸè®¡æ—¶
                end_time = time.perf_counter()
                
                inference_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                self.inference_times.append(inference_time)
                
                # è®°å½•å†…å­˜ä½¿ç”¨ï¼ˆæ¯100æ¬¡ï¼‰
                if (i + 1) % 100 == 0:
                    current_memory = process.memory_info().rss / 1024 / 1024  # MB
                    self.memory_usage.append(current_memory - initial_memory)
                
                # æ›´æ–°è¿›åº¦æ¡ï¼ˆæ¯10æ¬¡æ›´æ–°ä¸€æ¬¡ï¼‰
                if (i + 1) % 10 == 0 or i == num_iterations - 1:
                    if len(self.inference_times) > 0:
                        avg_time = np.mean(self.inference_times[-min(100, len(self.inference_times)):])
                        pbar.set_postfix({'avg_ms': f'{avg_time:.3f}'})
                    pbar.update(10 if (i + 1) % 10 == 0 else (i + 1) % 10)
        
        # ç¡®ä¿è¿›åº¦æ¡å®Œæˆ
        if not pbar.n == num_iterations:
            pbar.update(num_iterations - pbar.n)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self._print_statistics()
    
    def benchmark_batch_inference(self, batch_sizes: list = [1, 4, 8, 16, 32]):
        """æ‰¹é‡æ¨ç†æ€§èƒ½æµ‹è¯•"""
        print(f"\nğŸ“Š æ‰¹é‡æ¨ç†æ€§èƒ½æµ‹è¯•")
        print("=" * 80)
        
        results = {}
        
        for batch_size in batch_sizes:
            print(f"\n  æµ‹è¯•æ‰¹é‡å¤§å°: {batch_size}")
            dummy_obs = np.random.randn(batch_size, 45).astype(np.float32)
            batch_times = []
            
            # é¢„çƒ­
            with tqdm(total=10, desc="    é¢„çƒ­", unit="iter", ncols=60, leave=False) as pbar:
                for _ in range(10):
                    _ = self.policy(dummy_obs)
                    pbar.update(1)
            
            # æµ‹è¯•ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
            num_iterations = 100
            with tqdm(total=num_iterations, desc="    æµ‹è¯•", unit="iter", ncols=60, leave=False,
                      bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:
                for _ in range(num_iterations):
                    start_time = time.perf_counter()
                    _ = self.policy(dummy_obs)
                    end_time = time.perf_counter()
                    
                    batch_time = (end_time - start_time) * 1000  # æ¯«ç§’
                    batch_times.append(batch_time)
                    
                    # æ›´æ–°è¿›åº¦æ¡ï¼ˆæ¯10æ¬¡æ›´æ–°ä¸€æ¬¡ï¼‰
                    if len(batch_times) % 10 == 0 or len(batch_times) == num_iterations:
                        if len(batch_times) > 0:
                            avg_time = np.mean(batch_times[-min(20, len(batch_times)):])
                            pbar.set_postfix({'avg_ms': f'{avg_time:.3f}'})
                        pbar.update(10 if len(batch_times) % 10 == 0 else len(batch_times) % 10)
            
            # ç¡®ä¿è¿›åº¦æ¡å®Œæˆ
            if len(batch_times) < num_iterations:
                pbar.update(num_iterations - len(batch_times))
            
            # è®¡ç®—ç»Ÿè®¡
            mean_time = np.mean(batch_times)
            std_time = np.std(batch_times)
            throughput = batch_size / (mean_time / 1000)  # æ ·æœ¬/ç§’
            time_per_sample = mean_time / batch_size  # å•æ ·æœ¬æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
            
            results[batch_size] = {
                'mean_ms': mean_time,
                'std_ms': std_time,
                'throughput': throughput,
                'time_per_sample_ms': time_per_sample
            }
            
            print(f"    å¹³å‡æ—¶é—´: {mean_time:.3f} ms Â± {std_time:.3f} ms")
            print(f"    ååé‡: {throughput:.1f} æ ·æœ¬/ç§’")
            print(f"    å•æ ·æœ¬æ—¶é—´: {time_per_sample:.6f} ms ({time_per_sample*1000:.3f} Î¼s)")
        
        return results
    
    def _print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        if not self.inference_times:
            return
        
        times_ms = np.array(self.inference_times)
        
        # åŸºæœ¬ç»Ÿè®¡
        mean_ms = np.mean(times_ms)
        std_ms = np.std(times_ms)
        min_ms = np.min(times_ms)
        max_ms = np.max(times_ms)
        median_ms = np.median(times_ms)
        
        # ç™¾åˆ†ä½æ•°
        p50 = np.percentile(times_ms, 50)
        p95 = np.percentile(times_ms, 95)
        p99 = np.percentile(times_ms, 99)
        
        # FPS
        fps = 1000.0 / mean_ms if mean_ms > 0 else 0
        
        # å†…å­˜ä½¿ç”¨
        avg_memory = np.mean(self.memory_usage) if self.memory_usage else 0
        max_memory = np.max(self.memory_usage) if self.memory_usage else 0
        
        print(f"\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡ ({self.device_name}):")
        print(f"  â±ï¸  æ¨ç†æ—¶é—´:")
        print(f"     - å¹³å‡: {mean_ms:.3f} ms Â± {std_ms:.3f} ms")
        print(f"     - ä¸­ä½æ•°: {median_ms:.3f} ms")
        print(f"     - æœ€å°: {min_ms:.3f} ms")
        print(f"     - æœ€å¤§: {max_ms:.3f} ms")
        print(f"     - P50: {p50:.3f} ms")
        print(f"     - P95: {p95:.3f} ms")
        print(f"     - P99: {p99:.3f} ms")
        print(f"  ğŸš€ ååé‡:")
        print(f"     - FPS: {fps:.1f} å¸§/ç§’")
        print(f"     - ååé‡: {1000.0/mean_ms:.1f} æ¨ç†/ç§’")
        if avg_memory > 0:
            print(f"  ğŸ’¾ å†…å­˜ä½¿ç”¨:")
            print(f"     - å¹³å‡: {avg_memory:.1f} MB")
            print(f"     - æœ€å¤§: {max_memory:.1f} MB")
    
    def get_summary(self):
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if not self.inference_times:
            return None
        
        times_ms = np.array(self.inference_times)
        mean_ms = np.mean(times_ms)
        fps = 1000.0 / mean_ms if mean_ms > 0 else 0
        
        return {
            'device': self.device_name,
            'mean_ms': mean_ms,
            'std_ms': np.std(times_ms),
            'p50_ms': np.percentile(times_ms, 50),
            'p95_ms': np.percentile(times_ms, 95),
            'p99_ms': np.percentile(times_ms, 99),
            'fps': fps,
            'throughput': 1000.0 / mean_ms
        }


def benchmark_device(device: str, policy_path: str, warmup: int, iterations: int, batch_sizes: list):
    """å¯¹å•ä¸ªè®¾å¤‡è¿›è¡Œæ€§èƒ½æµ‹è¯•"""
    print(f"\n{'='*80}")
    print(f"ğŸ”¬ æ€§èƒ½æµ‹è¯•: {device.upper()}")
    print(f"{'='*80}")
    
    try:
        # åŠ è½½ç­–ç•¥
        policy = load_onnx_policy(policy_path, device=device)
        print(f"âœ“ ç­–ç•¥å·²åŠ è½½")
        print(f"  è®¾å¤‡: {device.upper()}")
        print(f"  Providers: {policy.session.get_providers()}")
        
        # åˆ›å»ºæµ‹è¯•å¯¹è±¡
        benchmark = Lite3PerformanceBenchmark(policy, device)
        
        # é¢„çƒ­
        benchmark.warmup(num_warmup=warmup)
        
        # å•æ¬¡æ¨ç†æµ‹è¯•
        benchmark.benchmark_single_inference(num_iterations=iterations)
        
        # æ‰¹é‡æ¨ç†æµ‹è¯•
        if batch_sizes:
            benchmark.benchmark_batch_inference(batch_sizes=batch_sizes)
        
        # æ¸…ç†
        del policy
        gc.collect()
        
        return benchmark.get_summary()
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def compare_all_devices(policy_path: str, warmup: int, iterations: int, export_json: str = None):
    """å¯¹æ¯”æ‰€æœ‰å¯ç”¨è®¾å¤‡"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š è®¾å¤‡æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print(f"{'='*80}")
    
    # æ£€æµ‹å¯ç”¨è®¾å¤‡
    available_devices = []
    
    # æ£€æŸ¥ MUSA
    try:
        import torch_musa
        if torch.musa.is_available():
            available_devices.append("musa")
    except:
        pass
    
    # æ£€æŸ¥ CUDA
    try:
        import torch
        if torch.cuda.is_available():
            available_devices.append("cuda")
    except:
        pass
    
    # CPU æ€»æ˜¯å¯ç”¨
    available_devices.append("cpu")
    
    print(f"\nå¯ç”¨è®¾å¤‡: {', '.join(available_devices)}")
    
    # æµ‹è¯•æ¯ä¸ªè®¾å¤‡
    results = {}
    for device in available_devices:
        summary = benchmark_device(device, policy_path, warmup, iterations, batch_sizes=[])
        if summary:
            results[device] = summary
        
        # è®¾å¤‡ä¹‹é—´ç¨ä½œå»¶è¿Ÿ
        time.sleep(1)
    
    # æ‰“å°å¯¹æ¯”ç»“æœ
    if len(results) > 1:
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“")
        print(f"{'='*80}")
        print(f"{'è®¾å¤‡':<10} {'å¹³å‡æ—¶é—´(ms)':<15} {'FPS':<10} {'ååé‡(æ¨ç†/ç§’)':<20} {'P95(ms)':<10} {'P99(ms)':<10}")
        print(f"{'-'*90}")
        
        # æŒ‰å¹³å‡æ—¶é—´æ’åº
        sorted_results = sorted(results.items(), key=lambda x: x[1]['mean_ms'])
        
        for device, summary in sorted_results:
            print(f"{device.upper():<10} {summary['mean_ms']:<15.3f} {summary['fps']:<10.1f} {summary['throughput']:<20.1f} {summary['p95_ms']:<10.3f} {summary['p99_ms']:<10.3f}")
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        if len(sorted_results) > 1:
            baseline = sorted_results[0][1]['mean_ms']  # æœ€å¿«è®¾å¤‡
            print(f"\nåŠ é€Ÿæ¯” (ç›¸å¯¹äºæœ€å¿«è®¾å¤‡):")
            for device, summary in sorted_results:
                speedup = baseline / summary['mean_ms']
                print(f"  {device.upper()}: {speedup:.2f}x")
            
            # è¯¦ç»†åˆ†æ
            print(f"\nğŸ“ˆ æ€§èƒ½åˆ†æ:")
            fastest = sorted_results[0]
            print(f"  - æœ€å¿«è®¾å¤‡: {fastest[0].upper()} ({fastest[1]['mean_ms']:.3f} ms)")
            
            if len(sorted_results) > 1:
                second = sorted_results[1]
                ratio = second[1]['mean_ms'] / fastest[1]['mean_ms']
                print(f"  - ç¬¬äºŒå¿«è®¾å¤‡: {second[0].upper()} ({second[1]['mean_ms']:.3f} ms, {ratio:.2f}x æ…¢)")
            
            # å»¶è¿Ÿç¨³å®šæ€§åˆ†æ
            print(f"\nâ±ï¸  å»¶è¿Ÿç¨³å®šæ€§ (P99/P50 æ¯”å€¼ï¼Œè¶Šå°è¶Šç¨³å®š):")
            for device, summary in sorted_results:
                stability = summary['p99_ms'] / summary['p50_ms'] if summary['p50_ms'] > 0 else float('inf')
                print(f"  - {device.upper()}: {stability:.2f}x")
        
        # å¯¼å‡º JSONï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if export_json:
            import json
            export_data = {
                'test_config': {
                    'warmup': warmup,
                    'iterations': iterations,
                    'policy_path': policy_path
                },
                'results': results,
                'summary': {
                    'fastest_device': sorted_results[0][0] if sorted_results else None,
                    'speedup_ratios': {
                        device: baseline / summary['mean_ms'] 
                        for device, summary in sorted_results
                    } if len(sorted_results) > 1 else {}
                }
            }
            with open(export_json, 'w') as f:
                json.dump(export_data, f, indent=2)
            print(f"\nğŸ’¾ ç»“æœå·²å¯¼å‡ºåˆ°: {export_json}")


def main():
    parser = argparse.ArgumentParser(description="Lite3 ONNX æ€§èƒ½æµ‹è¯•è„šæœ¬")
    parser.add_argument("--device", type=str, choices=['cpu', 'cuda', 'musa', 'auto'], 
                       default='auto', help="æµ‹è¯•è®¾å¤‡ (é»˜è®¤: auto)")
    parser.add_argument("--policy_path", type=str, 
                       default=None, help="ç­–ç•¥æ–‡ä»¶è·¯å¾„ (é»˜è®¤: policy.onnx)")
    parser.add_argument("--warmup", type=int, default=100, 
                       help="é¢„çƒ­è¿­ä»£æ¬¡æ•° (é»˜è®¤: 100)")
    parser.add_argument("--iterations", type=int, default=1000, 
                       help="æµ‹è¯•è¿­ä»£æ¬¡æ•° (é»˜è®¤: 1000)")
    parser.add_argument("--batch_sizes", type=int, nargs='+', default=[1, 4, 8, 16, 32],
                       help="æ‰¹é‡æ¨ç†æµ‹è¯•çš„æ‰¹é‡å¤§å° (é»˜è®¤: 1 4 8 16 32)")
    parser.add_argument("--compare_all", action='store_true',
                       help="å¯¹æ¯”æ‰€æœ‰å¯ç”¨è®¾å¤‡çš„æ€§èƒ½")
    parser.add_argument("--no_batch", action='store_true',
                       help="è·³è¿‡æ‰¹é‡æ¨ç†æµ‹è¯•")
    parser.add_argument("--export_json", type=str, default=None,
                       help="å¯¼å‡ºç»“æœåˆ° JSON æ–‡ä»¶")
    
    args = parser.parse_args()
    
    # é»˜è®¤ç­–ç•¥è·¯å¾„
    if args.policy_path is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        args.policy_path = os.path.join(script_dir, "policy.onnx")
    
    if not os.path.exists(args.policy_path):
        print(f"âŒ ç­–ç•¥æ–‡ä»¶ä¸å­˜åœ¨: {args.policy_path}")
        return
    
    print("="*80)
    print("ğŸš€ Lite3 ONNX æ€§èƒ½æµ‹è¯•")
    print("="*80)
    print(f"ç­–ç•¥æ–‡ä»¶: {args.policy_path}")
    print(f"é¢„çƒ­è¿­ä»£: {args.warmup}")
    print(f"æµ‹è¯•è¿­ä»£: {args.iterations}")
    
    # æ‰“å° GPU ä¿¡æ¯
    print_gpu_info()
    
    if args.compare_all:
        # å¯¹æ¯”æ‰€æœ‰è®¾å¤‡
        compare_all_devices(args.policy_path, args.warmup, args.iterations, args.export_json)
    else:
        # æµ‹è¯•å•ä¸ªè®¾å¤‡
        batch_sizes = [] if args.no_batch else args.batch_sizes
        benchmark_device(args.device, args.policy_path, args.warmup, args.iterations, batch_sizes)


if __name__ == "__main__":
    main()

