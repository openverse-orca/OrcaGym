#!/usr/bin/env python3
"""
XBot æ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•ä¸åŒè®¾å¤‡ï¼ˆMUSA GPU, CUDA GPU, CPUï¼‰çš„æ¨ç†æ€§èƒ½

ä½¿ç”¨æ–¹æ³•:
    python run_xbot_benchmark.py --device musa --warmup 100 --iterations 1000
    python run_xbot_benchmark.py --device auto --compare_all  # å¯¹æ¯”æ‰€æœ‰å¯ç”¨è®¾å¤‡
"""

import sys
import os
import time
import argparse
import numpy as np
import torch
from collections import deque
import statistics
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

from envs.xbot_gym.xbot_simple_env import XBotSimpleEnv
from orca_gym.utils.device_utils import get_torch_device, get_gpu_info, print_gpu_info
import psutil
import gc


class PerformanceBenchmark:
    """æ€§èƒ½æµ‹è¯•ç±»"""
    
    def __init__(self, policy, torch_device, device_name: str):
        self.policy = policy
        self.torch_device = torch_device
        self.device_name = device_name
        self.inference_times = []
        self.memory_usage = []
        
    def warmup(self, num_warmup: int = 100):
        """é¢„çƒ­ï¼šè¿è¡Œå¤šæ¬¡æ¨ç†ä»¥ç¨³å®šæ€§èƒ½"""
        print(f"ğŸ”¥ é¢„çƒ­ä¸­... ({num_warmup} æ¬¡æ¨ç†)")
        dummy_obs = np.random.randn(705).astype(np.float32)
        
        for _ in range(num_warmup):
            with torch.no_grad():
                obs_tensor = torch.from_numpy(dummy_obs).float().to(self.torch_device)
                _ = self.policy(obs_tensor)
        
        # åŒæ­¥ GPUï¼ˆå¦‚æœæœ‰ï¼‰
        if "musa" in str(self.torch_device) or "cuda" in str(self.torch_device):
            if "musa" in str(self.torch_device):
                torch.musa.synchronize()
            else:
                torch.cuda.synchronize()
        
        print("âœ“ é¢„çƒ­å®Œæˆ")
    
    def benchmark_single_inference(self, num_iterations: int = 1000):
        """å•æ¬¡æ¨ç†æ€§èƒ½æµ‹è¯•"""
        print(f"\nğŸ“Š å•æ¬¡æ¨ç†æ€§èƒ½æµ‹è¯• ({num_iterations} æ¬¡è¿­ä»£)")
        print("=" * 80)
        
        dummy_obs = np.random.randn(705).astype(np.float32)
        self.inference_times = []
        
        # è®°å½•åˆå§‹å†…å­˜
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æµ‹è¯•å¾ªç¯
        for i in range(num_iterations):
            # å¼€å§‹è®¡æ—¶
            if "musa" in str(self.torch_device) or "cuda" in str(self.torch_device):
                if "musa" in str(self.torch_device):
                    torch.musa.synchronize()
                else:
                    torch.cuda.synchronize()
                start_time = time.perf_counter()
            else:
                start_time = time.perf_counter()
            
            # æ¨ç†
            with torch.no_grad():
                obs_tensor = torch.from_numpy(dummy_obs).float().to(self.torch_device)
                action_tensor = self.policy(obs_tensor)
                action = action_tensor.cpu().numpy()
            
            # ç»“æŸè®¡æ—¶
            if "musa" in str(self.torch_device) or "cuda" in str(self.torch_device):
                if "musa" in str(self.torch_device):
                    torch.musa.synchronize()
                else:
                    torch.cuda.synchronize()
                end_time = time.perf_counter()
            else:
                end_time = time.perf_counter()
            
            inference_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            self.inference_times.append(inference_time)
            
            # è®°å½•å†…å­˜ä½¿ç”¨ï¼ˆæ¯100æ¬¡ï¼‰
            if (i + 1) % 100 == 0:
                current_memory = process.memory_info().rss / 1024 / 1024  # MB
                self.memory_usage.append(current_memory - initial_memory)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self._print_statistics()
    
    def benchmark_batch_inference(self, batch_sizes: list = [1, 4, 8, 16, 32]):
        """æ‰¹é‡æ¨ç†æ€§èƒ½æµ‹è¯•"""
        print(f"\nğŸ“Š æ‰¹é‡æ¨ç†æ€§èƒ½æµ‹è¯•")
        print("=" * 80)
        
        results = {}
        
        for batch_size in batch_sizes:
            print(f"\n  æµ‹è¯•æ‰¹é‡å¤§å°: {batch_size}")
            dummy_obs = np.random.randn(batch_size, 705).astype(np.float32)
            batch_times = []
            
            # é¢„çƒ­
            for _ in range(10):
                with torch.no_grad():
                    obs_tensor = torch.from_numpy(dummy_obs).float().to(self.torch_device)
                    _ = self.policy(obs_tensor)
            
            # åŒæ­¥
            if "musa" in str(self.torch_device) or "cuda" in str(self.torch_device):
                if "musa" in str(self.torch_device):
                    torch.musa.synchronize()
                else:
                    torch.cuda.synchronize()
            
            # æµ‹è¯•
            num_iterations = 100
            for _ in range(num_iterations):
                if "musa" in str(self.torch_device) or "cuda" in str(self.torch_device):
                    if "musa" in str(self.torch_device):
                        torch.musa.synchronize()
                    else:
                        torch.cuda.synchronize()
                    start_time = time.perf_counter()
                else:
                    start_time = time.perf_counter()
                
                with torch.no_grad():
                    obs_tensor = torch.from_numpy(dummy_obs).float().to(self.torch_device)
                    action_tensor = self.policy(obs_tensor)
                    _ = action_tensor.cpu().numpy()
                
                if "musa" in str(self.torch_device) or "cuda" in str(self.torch_device):
                    if "musa" in str(self.torch_device):
                        torch.musa.synchronize()
                    else:
                        torch.cuda.synchronize()
                    end_time = time.perf_counter()
                else:
                    end_time = time.perf_counter()
                
                batch_time = (end_time - start_time) * 1000  # æ¯«ç§’
                batch_times.append(batch_time)
            
            # è®¡ç®—ç»Ÿè®¡
            mean_time = np.mean(batch_times)
            std_time = np.std(batch_times)
            throughput = batch_size / (mean_time / 1000)  # æ ·æœ¬/ç§’
            time_per_sample = mean_time / batch_size  # å•æ ·æœ¬æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
            
            results[batch_size] = {
                'mean_ms': mean_time,
                'std_ms': std_time,
                'throughput': throughput,
                'time_per_sample_ms': time_per_sample
            }
            
            print(f"    å¹³å‡æ—¶é—´: {mean_time:.3f} ms Â± {std_time:.3f} ms")
            print(f"    ååé‡: {throughput:.1f} æ ·æœ¬/ç§’")
            print(f"    å•æ ·æœ¬æ—¶é—´: {time_per_sample:.6f} ms ({time_per_sample*1000:.3f} Î¼s)")
        
        return results
    
    def _print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        if not self.inference_times:
            return
        
        times_ms = np.array(self.inference_times)
        
        # åŸºæœ¬ç»Ÿè®¡
        mean_ms = np.mean(times_ms)
        std_ms = np.std(times_ms)
        min_ms = np.min(times_ms)
        max_ms = np.max(times_ms)
        median_ms = np.median(times_ms)
        
        # ç™¾åˆ†ä½æ•°
        p50 = np.percentile(times_ms, 50)
        p95 = np.percentile(times_ms, 95)
        p99 = np.percentile(times_ms, 99)
        
        # FPS
        fps = 1000.0 / mean_ms if mean_ms > 0 else 0
        
        # å†…å­˜ä½¿ç”¨
        avg_memory = np.mean(self.memory_usage) if self.memory_usage else 0
        max_memory = np.max(self.memory_usage) if self.memory_usage else 0
        
        print(f"\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡ ({self.device_name}):")
        print(f"  â±ï¸  æ¨ç†æ—¶é—´:")
        print(f"     - å¹³å‡: {mean_ms:.3f} ms Â± {std_ms:.3f} ms")
        print(f"     - ä¸­ä½æ•°: {median_ms:.3f} ms")
        print(f"     - æœ€å°: {min_ms:.3f} ms")
        print(f"     - æœ€å¤§: {max_ms:.3f} ms")
        print(f"     - P50: {p50:.3f} ms")
        print(f"     - P95: {p95:.3f} ms")
        print(f"     - P99: {p99:.3f} ms")
        print(f"  ğŸš€ ååé‡:")
        print(f"     - FPS: {fps:.1f} å¸§/ç§’")
        print(f"     - ååé‡: {1000.0/mean_ms:.1f} æ¨ç†/ç§’")
        if avg_memory > 0:
            print(f"  ğŸ’¾ å†…å­˜ä½¿ç”¨:")
            print(f"     - å¹³å‡: {avg_memory:.1f} MB")
            print(f"     - æœ€å¤§: {max_memory:.1f} MB")
    
    def get_summary(self):
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if not self.inference_times:
            return None
        
        times_ms = np.array(self.inference_times)
        mean_ms = np.mean(times_ms)
        fps = 1000.0 / mean_ms if mean_ms > 0 else 0
        
        return {
            'device': self.device_name,
            'mean_ms': mean_ms,
            'std_ms': np.std(times_ms),
            'p50_ms': np.percentile(times_ms, 50),
            'p95_ms': np.percentile(times_ms, 95),
            'p99_ms': np.percentile(times_ms, 99),
            'fps': fps,
            'throughput': 1000.0 / mean_ms
        }


def load_xbot_policy(policy_path: str, device: str = "auto"):
    """åŠ è½½XBotç­–ç•¥"""
    if device == "auto":
        torch_device = get_torch_device(try_to_use_gpu=True)
        device_str = str(torch_device)
        if "musa" in device_str:
            device = "musa"
        elif "cuda" in device_str:
            device = "cuda"
        else:
            device = "cpu"
    else:
        if device == "musa":
            try:
                import torch_musa
                if torch.musa.is_available():
                    torch_device = torch.device("musa:0")
                else:
                    raise RuntimeError("MUSA GPU not available")
            except ImportError:
                raise RuntimeError("torch_musa not installed")
        elif device == "cuda":
            if torch.cuda.is_available():
                torch_device = torch.device("cuda:0")
            else:
                raise RuntimeError("CUDA not available")
        else:
            torch_device = torch.device("cpu")
    
    policy = torch.jit.load(policy_path, map_location=torch_device)
    policy.eval()
    policy.to(torch_device)
    
    return policy, torch_device, device


def benchmark_device(device: str, policy_path: str, warmup: int, iterations: int, batch_sizes: list):
    """å¯¹å•ä¸ªè®¾å¤‡è¿›è¡Œæ€§èƒ½æµ‹è¯•"""
    print(f"\n{'='*80}")
    print(f"ğŸ”¬ æ€§èƒ½æµ‹è¯•: {device.upper()}")
    print(f"{'='*80}")
    
    try:
        # åŠ è½½ç­–ç•¥
        policy, torch_device, device_name = load_xbot_policy(policy_path, device)
        print(f"âœ“ ç­–ç•¥å·²åŠ è½½åˆ°è®¾å¤‡: {torch_device}")
        
        # åˆ›å»ºæµ‹è¯•å¯¹è±¡
        benchmark = PerformanceBenchmark(policy, torch_device, device_name)
        
        # é¢„çƒ­
        benchmark.warmup(num_warmup=warmup)
        
        # å•æ¬¡æ¨ç†æµ‹è¯•
        benchmark.benchmark_single_inference(num_iterations=iterations)
        
        # æ‰¹é‡æ¨ç†æµ‹è¯•
        if batch_sizes:
            benchmark.benchmark_batch_inference(batch_sizes=batch_sizes)
        
        # æ¸…ç†
        del policy
        gc.collect()
        if "musa" in str(torch_device) or "cuda" in str(torch_device):
            if "musa" in str(torch_device):
                torch.musa.empty_cache()
            else:
                torch.cuda.empty_cache()
        
        return benchmark.get_summary()
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return None


def compare_all_devices(policy_path: str, warmup: int, iterations: int, export_json: str = None):
    """å¯¹æ¯”æ‰€æœ‰å¯ç”¨è®¾å¤‡"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š è®¾å¤‡æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print(f"{'='*80}")
    
    # æ£€æµ‹å¯ç”¨è®¾å¤‡
    available_devices = []
    
    # æ£€æŸ¥ MUSA
    try:
        import torch_musa
        if torch.musa.is_available():
            available_devices.append("musa")
    except:
        pass
    
    # æ£€æŸ¥ CUDA
    if torch.cuda.is_available():
        available_devices.append("cuda")
    
    # CPU æ€»æ˜¯å¯ç”¨
    available_devices.append("cpu")
    
    print(f"\nå¯ç”¨è®¾å¤‡: {', '.join(available_devices)}")
    
    # æµ‹è¯•æ¯ä¸ªè®¾å¤‡
    results = {}
    for device in available_devices:
        summary = benchmark_device(device, policy_path, warmup, iterations, batch_sizes=[])
        if summary:
            results[device] = summary
        
        # è®¾å¤‡ä¹‹é—´ç¨ä½œå»¶è¿Ÿ
        time.sleep(1)
    
    # æ‰“å°å¯¹æ¯”ç»“æœ
    if len(results) > 1:
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“")
        print(f"{'='*80}")
        print(f"{'è®¾å¤‡':<10} {'å¹³å‡æ—¶é—´(ms)':<15} {'FPS':<10} {'ååé‡(æ¨ç†/ç§’)':<20} {'P95(ms)':<10} {'P99(ms)':<10}")
        print(f"{'-'*90}")
        
        # æŒ‰å¹³å‡æ—¶é—´æ’åº
        sorted_results = sorted(results.items(), key=lambda x: x[1]['mean_ms'])
        
        for device, summary in sorted_results:
            print(f"{device.upper():<10} {summary['mean_ms']:<15.3f} {summary['fps']:<10.1f} {summary['throughput']:<20.1f} {summary['p95_ms']:<10.3f} {summary['p99_ms']:<10.3f}")
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        if len(sorted_results) > 1:
            baseline = sorted_results[0][1]['mean_ms']  # æœ€å¿«è®¾å¤‡
            print(f"\nåŠ é€Ÿæ¯” (ç›¸å¯¹äºæœ€å¿«è®¾å¤‡):")
            for device, summary in sorted_results:
                speedup = baseline / summary['mean_ms']
                print(f"  {device.upper()}: {speedup:.2f}x")
            
            # è¯¦ç»†åˆ†æ
            print(f"\nğŸ“ˆ æ€§èƒ½åˆ†æ:")
            fastest = sorted_results[0]
            print(f"  - æœ€å¿«è®¾å¤‡: {fastest[0].upper()} ({fastest[1]['mean_ms']:.3f} ms)")
            
            if len(sorted_results) > 1:
                second = sorted_results[1]
                ratio = second[1]['mean_ms'] / fastest[1]['mean_ms']
                print(f"  - ç¬¬äºŒå¿«è®¾å¤‡: {second[0].upper()} ({second[1]['mean_ms']:.3f} ms, {ratio:.2f}x æ…¢)")
            
            # å»¶è¿Ÿç¨³å®šæ€§åˆ†æ
            print(f"\nâ±ï¸  å»¶è¿Ÿç¨³å®šæ€§ (P99/P50 æ¯”å€¼ï¼Œè¶Šå°è¶Šç¨³å®š):")
            for device, summary in sorted_results:
                stability = summary['p99_ms'] / summary['p50_ms'] if summary['p50_ms'] > 0 else float('inf')
                print(f"  - {device.upper()}: {stability:.2f}x")
        
        # å¯¼å‡º JSONï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if export_json:
            import json
            export_data = {
                'test_config': {
                    'warmup': warmup,
                    'iterations': iterations,
                    'policy_path': policy_path
                },
                'results': results,
                'summary': {
                    'fastest_device': sorted_results[0][0] if sorted_results else None,
                    'speedup_ratios': {
                        device: baseline / summary['mean_ms'] 
                        for device, summary in sorted_results
                    } if len(sorted_results) > 1 else {}
                }
            }
            with open(export_json, 'w') as f:
                json.dump(export_data, f, indent=2)
            print(f"\nğŸ’¾ ç»“æœå·²å¯¼å‡ºåˆ°: {export_json}")


def main():
    parser = argparse.ArgumentParser(description="XBot æ€§èƒ½æµ‹è¯•è„šæœ¬")
    parser.add_argument("--device", type=str, choices=['cpu', 'cuda', 'musa', 'auto'], 
                       default='auto', help="æµ‹è¯•è®¾å¤‡ (é»˜è®¤: auto)")
    parser.add_argument("--policy_path", type=str, 
                       default=None, help="ç­–ç•¥æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config/policy_example.pt)")
    parser.add_argument("--warmup", type=int, default=100, 
                       help="é¢„çƒ­è¿­ä»£æ¬¡æ•° (é»˜è®¤: 100)")
    parser.add_argument("--iterations", type=int, default=1000, 
                       help="æµ‹è¯•è¿­ä»£æ¬¡æ•° (é»˜è®¤: 1000)")
    parser.add_argument("--batch_sizes", type=int, nargs='+', default=[1, 4, 8, 16, 32],
                       help="æ‰¹é‡æ¨ç†æµ‹è¯•çš„æ‰¹é‡å¤§å° (é»˜è®¤: 1 4 8 16 32)")
    parser.add_argument("--compare_all", action='store_true',
                       help="å¯¹æ¯”æ‰€æœ‰å¯ç”¨è®¾å¤‡çš„æ€§èƒ½")
    parser.add_argument("--no_batch", action='store_true',
                       help="è·³è¿‡æ‰¹é‡æ¨ç†æµ‹è¯•")
    parser.add_argument("--export_json", type=str, default=None,
                       help="å¯¼å‡ºç»“æœåˆ° JSON æ–‡ä»¶")
    
    args = parser.parse_args()
    
    # é»˜è®¤ç­–ç•¥è·¯å¾„
    if args.policy_path is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        args.policy_path = os.path.join(script_dir, "config", "policy_example.pt")
    
    if not os.path.exists(args.policy_path):
        print(f"âŒ ç­–ç•¥æ–‡ä»¶ä¸å­˜åœ¨: {args.policy_path}")
        return
    
    print("="*80)
    print("ğŸš€ XBot æ€§èƒ½æµ‹è¯•")
    print("="*80)
    print(f"ç­–ç•¥æ–‡ä»¶: {args.policy_path}")
    print(f"é¢„çƒ­è¿­ä»£: {args.warmup}")
    print(f"æµ‹è¯•è¿­ä»£: {args.iterations}")
    
    # æ‰“å° GPU ä¿¡æ¯
    print_gpu_info()
    
    if args.compare_all:
        # å¯¹æ¯”æ‰€æœ‰è®¾å¤‡
        compare_all_devices(args.policy_path, args.warmup, args.iterations, args.export_json)
    else:
        # æµ‹è¯•å•ä¸ªè®¾å¤‡
        batch_sizes = [] if args.no_batch else args.batch_sizes
        benchmark_device(args.device, args.policy_path, args.warmup, args.iterations, batch_sizes)


if __name__ == "__main__":
    main()

